{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "- Gradio is a UI-building library that enables quick prototyping and deployment of ML applications.\n",
    "- It was originally a startup and later acquired by Hugging Face.\n",
    "- The main goal is to create delightful and interactive ML apps with minimal effort."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shout(text):\n",
    "    \"\"\"\n",
    "    This function takes a string and returns it in uppercase with an exclamation mark at the end.\n",
    "    \"\"\"\n",
    "    return text.upper() + \"!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7860\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created dataset file at: .gradio\\flagged\\dataset1.csv\n"
     ]
    }
   ],
   "source": [
    "gr.Interface(\n",
    "    fn=shout,\n",
    "    inputs=gr.Textbox(label=\"Enter text to shout:\"),\n",
    "    outputs=gr.Textbox(label=\"Response:\"),\n",
    "    title=\"Shout It Out!\",\n",
    "    description=\"Type something and hear it shouted back at you!\",\n",
    ").launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Flag\n",
    "- The `flag` parameter is used in the flagging system, which allows users to save specific inputs and outputs from an interface. \n",
    "- This is useful for data collection, debugging, and tracking model errors.\n",
    "- Flagged data is saved in a CSV file (by default) in the working directory.\n",
    "- Default is enabled. You can disable it by passing `allow_flagging` as False.\n",
    "- Modes\n",
    "  - `manual` → Users manually click the flag button.\n",
    "  - `auto` → Automatically flags every input-output pair.\n",
    "  - `never` → Disables flagging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "openai = OpenAI(base_url=\"http://127.0.0.1:11434/v1\", api_key=\"ollama\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"You are a helpful assistant.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def message_gpt(prompt, model):\n",
    "    response = openai.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[\n",
    "            { \"role\": \"system\", \"content\": system_prompt },\n",
    "            { \"role\": \"user\", \"content\": prompt },\n",
    "        ]\n",
    "    )\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\TFS\\Study\\machine-learning\\.venv\\lib\\site-packages\\gradio\\interface.py:403: UserWarning: The `allow_flagging` parameter in `Interface` is deprecated.Use `flagging_mode` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7865\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7865/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from matplotlib.pyplot import flag\n",
    "\n",
    "\n",
    "gr.Interface(\n",
    "    fn=message_gpt,\n",
    "    inputs=[gr.Textbox(label=\"Enter your message:\"), gr.Dropdown(label=\"Select model:\", choices=[\"llama3.2\", \"gpt-4o\"])],\n",
    "    outputs=gr.Textbox(label=\"Response from GPT:\"),\n",
    "    title=\"Chat with GPT\",\n",
    "    description=\"Type a message and get a response from the GPT model.\",\n",
    "    allow_flagging=\"never\"\n",
    ").launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Streaming with Gradio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stream_with_gpt(prompt, model):\n",
    "    response = openai.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[\n",
    "            { \"role\": \"system\", \"content\": system_prompt },\n",
    "            { \"role\": \"user\", \"content\": prompt },\n",
    "        ],\n",
    "        stream=True\n",
    "    )\n",
    "    \n",
    "    # Note - Below is the standard code for getting streamed chunks from the response. \n",
    "    #for chunk in response:\n",
    "    #    yield chunk.choices[0].delta.content or \"\"\n",
    "\n",
    "    # However, gradio expects full content to be returned, so we need to collect the chunks \n",
    "    # and return them as a generator (using yield).\n",
    "    result = \"\"\n",
    "    for chunk in response:\n",
    "        result += chunk.choices[0].delta.content or \"\"\n",
    "        yield result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\TFS\\Study\\machine-learning\\.venv\\lib\\site-packages\\gradio\\interface.py:403: UserWarning: The `allow_flagging` parameter in `Interface` is deprecated.Use `flagging_mode` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7869\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7869/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gr.Interface(\n",
    "    fn=stream_with_gpt,\n",
    "    inputs=[gr.Textbox(label=\"Enter your message:\"), gr.Dropdown(label=\"Select model:\", choices=[\"llama3.2\", \"gpt-4o\"])],\n",
    "    outputs=gr.Textbox(label=\"Response from GPT:\"),\n",
    "    title=\"Stream with GPT\",\n",
    "    description=\"Type a message and get a streamed response from the GPT model.\",\n",
    "    allow_flagging=\"never\",\n",
    ").launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summarize company brochure with Gradio and Markdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "class Website:\n",
    "    def __init__(self, url):\n",
    "        self.url = url\n",
    "\n",
    "    def get_website_contents(self):\n",
    "        response = requests.get(self.url)\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        title = soup.title.string if soup.title else \"No title found\"\n",
    "        for irrelevant in soup(['script', 'style', 'img', 'input']):\n",
    "            irrelevant.decompose()\n",
    "        content = soup.body.get_text(separator='\\n', strip=True)\n",
    "        return title, content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_brochure(url, model):\n",
    "    website = Website(url)\n",
    "    title, content = website.get_website_contents()\n",
    "\n",
    "    system_prompt = f\"\"\"\n",
    "    You are an expert in creating company brochure given the URLs and content of it. \n",
    "    You create this brochure and return it in Markdown format.\"\"\"\n",
    "    \n",
    "    user_prompt = f\"\"\"\n",
    "        The website you have been provided is of {website.url}.\n",
    "        The title of the website is {title}.\n",
    "        The content of the website is as follows:\n",
    "        {content}\n",
    "        Using this data, you need to create a creative company brochure in Markdown format.\n",
    "    \"\"\"\n",
    "\n",
    "    \n",
    "    response = openai.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[\n",
    "            { \"role\": \"system\", \"content\": system_prompt },\n",
    "            { \"role\": \"user\", \"content\": user_prompt },\n",
    "        ]\n",
    "    )\n",
    "    return response.choices[0].message.content    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**Ollama: Unlocking the Power of Large Language Models**\n",
      "\n",
      "[Cover Image: A futuristic illustration of Llama, surrounded by code and connected devices]\n",
      "\n",
      "Welcome to Ollama\n",
      "================\n",
      "\n",
      "We empower developers, researchers, and innovators to unlock the full potential of large language models. Our cutting-edge technology provides a range of tools and frameworks to help you build, customize, and deploy your own models.\n",
      "\n",
      "**Our Models**\n",
      "---------------\n",
      "\n",
      "At Ollama, we are committed to delivering high-quality models that meet your specific needs. Our current offerings include:\n",
      "\n",
      "* **Llama 3.3**: A state-of-the-art model for natural language processing and generation.\n",
      "* **DeepSeek-R1**: A specialized model for deep learning applications.\n",
      "* **Phi-4**: A general-purpose model for a wide range of NLP tasks.\n",
      "* **Mistral**: A high-performance model for rapid prototyping and development.\n",
      "* **Gemma 3**: Our latest breakthrough in language modeling, designed for exceptional accuracy and speed.\n",
      "\n",
      "**Key Features**\n",
      "----------------\n",
      "\n",
      "* Run models locally on your preferred platform (macOS, Linux, Windows)\n",
      "* Explore our public repositories to learn more about each model\n",
      "* Collaborate with our community through Discord, GitHub, and Meetups\n",
      "\n",
      "**Get Started Today!**\n",
      "------------------------\n",
      "\n",
      "[Call-to-Action Button: Download]\n",
      "\n",
      "**Additional Resources**\n",
      "----------------------\n",
      "\n",
      "* **Blog**:Stay up-to-date on the latest news, research, and insights from Ollama's experts.\n",
      "* **Docs**:Access comprehensive documentation for our models, frameworks, and tools.\n",
      "\n",
      "**Connect with Us**\n",
      "------------------\n",
      "\n",
      "* **Discord**:Join our community forum to discuss model deployment, development, and best practices.\n",
      "* **GitHub**:Explore our publicly available repositories and contribute to our open-source projects.\n",
      "* **Twitter (X)**:Follow us @Ollama for the latest updates, news, and behind-the-scenes insights.\n",
      "\n",
      "**About Us**\n",
      "--------------\n",
      "\n",
      "At Ollama Inc., we are dedicated to advancing the field of natural language processing. Our mission is to democratize access to large language models, empowering innovators to create new and exciting applications.\n"
     ]
    }
   ],
   "source": [
    "brochure_content = get_brochure(\"https://www.ollama.com\", \"llama3.2\")\n",
    "print(brochure_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\TFS\\Study\\machine-learning\\.venv\\lib\\site-packages\\gradio\\interface.py:403: UserWarning: The `allow_flagging` parameter in `Interface` is deprecated.Use `flagging_mode` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7871\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7871/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gr.Interface(\n",
    "    fn=get_brochure,\n",
    "    inputs=[gr.Textbox(label=\"Enter the URL of the website:\"), gr.Dropdown(label=\"Select model:\", choices=[\"llama3.2\", \"gpt-4o\"])],\n",
    "    outputs=gr.Textbox(label=\"Brochure Content:\"),\n",
    "    title=\"Create Company Brochure\",\n",
    "    description=\"Enter a URL and get a company brochure in Markdown format.\",\n",
    "    allow_flagging=\"never\",\n",
    ").launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_brochure_stream(url, model):\n",
    "    website = Website(url)\n",
    "    title, content = website.get_website_contents()\n",
    "\n",
    "    system_prompt = f\"\"\"\n",
    "    You are an expert in creating company brochure given the URLs and content of it. \n",
    "    You create this brochure and return it in Markdown format.\"\"\"\n",
    "    \n",
    "    user_prompt = f\"\"\"\n",
    "        The website you have been provided is of {website.url}.\n",
    "        The title of the website is {title}.\n",
    "        The content of the website is as follows:\n",
    "        {content}\n",
    "        Using this data, you need to create a creative company brochure in Markdown format.\n",
    "    \"\"\"\n",
    "\n",
    "    \n",
    "    response = openai.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[\n",
    "            { \"role\": \"system\", \"content\": system_prompt },\n",
    "            { \"role\": \"user\", \"content\": user_prompt },\n",
    "        ],\n",
    "        stream=True\n",
    "    )\n",
    "\n",
    "    result = \"\"\n",
    "    for chunk in response:\n",
    "        result += chunk.choices[0].delta.content or \"\"\n",
    "        result = result.replace(\"```\", \"\").replace(\"markdown\", \"\")\n",
    "        yield result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\TFS\\Study\\machine-learning\\.venv\\lib\\site-packages\\gradio\\interface.py:403: UserWarning: The `allow_flagging` parameter in `Interface` is deprecated.Use `flagging_mode` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7873\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7873/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gr.Interface(\n",
    "    fn=get_brochure_stream,\n",
    "    inputs=[gr.Textbox(label=\"Enter the URL of the website:\"), gr.Dropdown(label=\"Select model:\", choices=[\"llama3.2\", \"gpt-4o\"])],\n",
    "    outputs=gr.Markdown(label=\"Brochure Content:\"),\n",
    "    title=\"Create Company Brochure\",\n",
    "    description=\"Enter a URL and get a company brochure in Markdown format.\",\n",
    "    allow_flagging=\"never\",\n",
    ").launch()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
