{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic Features to compare LLMs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " **Basic Parameters**                             | **Comments**                                                                                                      \n",
    "--------------------------------------------------|-------------------------------------------------------------------------------------------------------------------\n",
    " **Open-source / closed**                         | Impacts cost, licensing, and extensibility.                                                                       \n",
    " **Release date**                                 | When the model was first made available.                                                                          \n",
    " **Knowledge cut-off**                            | The last date included in its training data (after which it “knows” nothing new).                                 \n",
    " **Parameters**                                   | Total trainable weights—gives a rough sense of model capacity (and cost).                                         \n",
    " **Training tokens**                              | Size of the training corpus (in tokens)—indicates depth and breadth of learned knowledge.                         \n",
    " **Context Length**                               | Maximum number of input tokens the model can “see” at once (window size).                                         \n",
    " **Inference Cost**                               | Per‑query cost—API token pricing, subscription fees, or self‑hosted compute charges (e.g., Colab, GPU instances). \n",
    " **Training cost**                                | \tExpenses for fine‑tuning or full training (if you plan to adapt the model).                                      \n",
    " **Build cost**                                   | Development effort and engineering resources required to integrate and deploy the model.                          \n",
    " **Time to Market**                               | \tHow quickly you can have a working solution (frontier models often win here).                                    \n",
    " **Rate limits**                                  | API call quotas or throttling (especially on managed services/subscriptions).                                     \n",
    " **Speed**                                        | Tokens per second—how fast the model can generate complete outputs once running.                                  \n",
    " **Latency**                                      | Time to first token or end‑to‑end response time—critical for real‑time applications.                              \n",
    " **License**                                      | Usage and redistribution rights, commercial restrictions, and any revenue‑based clauses.                          \n",
    " **Ecosystem & Tooling Support**                  | Availability of SDKs, libraries, and integration plugins (e.g., Hugging Face Hub, LangChain).                     \n",
    " **Community & Documentation**                    | Quality of docs, tutorials, and community forums for troubleshooting and best practices.                          \n",
    " **Fine‑Tuning Capabilities**                     | Supported methods (LoRA, full‑model, adapters) and ease of use.                                                   \n",
    " **Hardware Requirements**                        | Minimum GPU/TPU specs for inference and training (memory, compute).                                               \n",
    " **Multi‑Modal Support**                          | Native ability to handle text+images, audio, code, etc.                                                           \n",
    " **Language Coverage & Multilingual Performance** | Number of supported languages and comparative benchmarks across them.                                             \n",
    " **Alignment & Safety Features**                  | Built‑in guardrails, content filters, and mitigation of harmful outputs.                                          \n",
    " **Privacy & Data Governance**                    | On‑prem vs. cloud hosting options, data retention policies, and compliance (e.g., GDPR, HIPAA).                   \n",
    " **Security & Compliance**                        | Certifications (SOC 2, ISO 27001) and enterprise‑grade security features.                                         \n",
    " **Interpretability & Explainability**             | Tools or APIs for understanding model decisions (attention visualization, token attribution).                     \n",
    " **Update Cadence**                               | Frequency of model improvements, patch releases, and knowledge base refreshes.                                    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Chinchilla Scaling Law"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### What Are “Scaling Laws”\n",
    "- **Definition**: Empirical relationships that predict how model performance (e.g., loss, accuracy) improves as you increase key resources—model size (parameters), dataset size (tokens), or compute (FLOPs).\n",
    "- **Why they matter**: Help researchers allocate a fixed compute budget most effectively between bigger models or more data.\n",
    "- Origin: Coined by Google DeepMind after their 70 B‑parameter model “Chinchilla”\n",
    "Core insight: Rather than simply increasing parameters, balance model size and data so that `D ~ 20 * N`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### The Chinchilla Law (High‑Level Statement)\n",
    "- “For a given compute budget, the optimal number of training tokens should scale linearly with model size, at roughly a 20 : 1 token‑to‑parameter ratio.”"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Model: Chinchilla is a 70 B‑parameter LLM trained on ~1.3 T tokens—4× more data than its 280 B‑parameter predecessor Gopher, yet using the same total FLOPs.\n",
    "- Key insight: Previous “bigger‑is‑better” approaches under‑utilized data; you get more bang‑for‑your‑buck by training somewhat smaller models on more tokens."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Simple Intuition & Analogy\n",
    "- Cookie‑baking\n",
    "    - You have a fixed oven‑time budget (compute).\n",
    "    - You can bake bigger cookies (larger model) or more smaller cookies (more data passes).\n",
    "    - Chinchilla says: bake moderately sized cookies but many of them—your total yield (performance) is maximized."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Basic Numerical Example\n",
    "Compute budget `C` is w.r.t (FLOPSs i.e. `Floating-point operations per second`)\n",
    "\n",
    " **Compute Budget (FLOPs)** | **Gopher (old)**       | **Chinchilla (optimal)** \n",
    "----------------------------|------------------------|--------------------------\n",
    " **Model size (N)**         | 280 B parameters       | 70 B parameters          \n",
    " **Tokens (D)**             | 300 B tokens           | 1.3 T tokens             \n",
    " **Tokens / Params (D/N)**  | ~1.1                   | ~18.6 (≈20 : 1)          \n",
    " **Outcome**                | Lower accuracy on MMLU | +7 % MMLU improvement    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Mathematical Formulation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](images/mathematical_formulation_chinchilla.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Advanced Insights\n",
    "- Loss Scaling\n",
    "    - Empirically, cross‑entropy loss `L` follows a power law in `N` and `D`.\n",
    "    - Chinchilla fits this law to find the minimum `L` at each `C`.\n",
    "- Beyond Pre‑Training\n",
    "    - For fine‑tuning or inference‑heavy workloads, newer work (e.g. “Beyond Chinchilla‑Optimal”) suggests slight shifts in the ratio to account for downstream costs.\n",
    "- Practical Takeaway\n",
    "    - If you have limited compute, don’t chase ever‑larger models—allocate more steps over more data instead."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Common Technical Benchmarks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " **Benchmark** | **Full Form**                                                                                        | **What's being evaluated** | **Description**                                                                                                       | **Intuitive Example**                                                                                  \n",
    "----------------|------------------------------------------------------------------------------------------------------|----------------------------|------------------------------------------------------------------------------------------------------------------------|--------------------------------------------------------------------------------------------------------\n",
    " **ARC**       | \tAI2 Reasoning Challenge                                                                             | Reasoning                  | A benchmark for evaluating scientific reasoning; multiple-choice questions                                             | “Which planet has a stronger gravitational pull, Earth or Mars?”                                       \n",
    " **DROP**       | Discrete Reasoning Over Paragraphs                                                                   | Language Comp              | Distill details from text then add, count or sort                                                                      | “From the paragraph, how many times does ‘Alice’ visit the park?”                                      \n",
    " **HellaSwag**  | Harder Endings, Longer contexts, and Low‑shot Activities for Situations With Adversarial Generations | Common Sense               | \"Harder Endings, Long Contexts and Low Shot Activities\"                                                                | “The child was hungry, so she opened the fridge and grabbed a ___.” (choices: “sandwich”, “pillow”, …) \n",
    " **MMLU**       | \tMeasuring Massive Multitask Language Understanding                                                  | Understanding              | Factual recall, reasoning and problem solving across 57 subjects                                                       | “Who wrote the U.S. Declaration of Independence?”<br>                                                  \n",
    " **TruthfulQA** | Truthful Question Answering: Measuring How Models Mimic Human Falsehoods                             | Accuracy                   | Robustness in providing truthful replies in adversarial conditions                                                     | “What is the capital of Atlantis?” (model should admit it’s mythical)                                  \n",
    " **Winogrande** | WinoGrande: Large‑scale Winograd Schema Challenge                                                    | Context                    | Test the LLM understands context and resolves ambiguity                                                                | “The trophy doesn’t fit in the brown suitcase because it’s too large. What is too large?”              \n",
    " **GSM8K**      | Grade School Math 8K                                                                                 | Math                       | Math and word problems taught in elementary and middle schools                                                         | “If you have 3 apples and buy 2 more, how many do you have total?”                                     \n",
    " **ELO**        | Elo Rating System (Chatbot Arena)                                                                    | Chat                       | Results from head-to-head face-offs with other LLMs, as with ELO in Chess                                              | “Users compare answers from Model A vs Model B and vote; winners gain Elo.”                            \n",
    " **HumanEval**  | HumanEval: Hand‑Written Evaluation Set                                                               | Python Coding              | 164 problems writing code based on docstrings                                                                          | “Write `def add(a, b):` that returns the sum of a and b.”                                                \n",
    " **MultiPL-E**  | Multiple Programming Languages Evaluation                                                            | Broader Coding             | Translation of HumanEval to 18 programming languages                                                                   | “Solve the same ‘add two numbers’ task in Python, JavaScript, and Rust.”                               \n",
    " **GPQA**       | \tGraduate‑Level Google‑Proof Q&A                                                                     | Graduate Tests             | 448 expert questions; non-PhD humans score 34% even with web access                                                    | “In quantum mechanics, what is the eigenvalue equation for the Hamiltonian?”                           \n",
    " **BBHard**     | BIG‑Bench Hard                                                                                       | Future Capabilities        | 204 tasks believed beyond capabilities of LLMs (no longer!)                                                            | “Translate English poetry into iambic pentameter with rhyme scheme ABAB.”                              \n",
    " **Math Lv 5**  | MATH Level 5                                                                                         | Math                       | High-school level math competition problems                                                                            | “Compute the integral ∫(3x² – 2x + 1) dx.”                                                             \n",
    " **IFEval**     | Instruction‑Following Evaluation                                                                     | Difficult instructions     | Like, \"write more than 400 words\" and \"mention AI at least 3 times\"                                                    | “Write a 500‑word essay on climate change and include the phrase ‘global warming’ at least 5 times.”   \n",
    " **MuSR**       | Multistep Soft Reasoning                                                                             | Multistep Soft Reasoning   | Logical deduction, such as analyzing 1,000 word murder mystery and answering: \"Who has means, motive and opportunity?\" | “Given a 1 000‑word detective story, who had means, motive, and opportunity to commit the crime?”      \n",
    " **MMLU-PRO**   | MMLU‑Pro: Professional Version of MMLU                                                               | Harder MMLU                | A more advanced and cleaned up version of MMLU including choice of 10 answers instead of 4                             | “From 10 possible U.S. history events, pick the one that triggered the Monroe Doctrine.”               \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Limitations of Benchmarking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Benchmarks drive research and purchasing decisions by giving \"scores\" to models—but those scores can be misleading if the benchmarks themselves are flawed or mis‑used. \n",
    "- Always interpret results in context, with healthy skepticism.\n",
    "- Following are the limitations:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inconsistent Application\n",
    "- What\n",
    "    - Different groups run the same benchmark under varying conditions (hardware, prompt templates, preprocessing).\n",
    "- Why it matters\n",
    "    - Scores aren’t directly comparable if one lab uses a beefy GPU cluster and another uses a single CPU. Companies may even tune a special variant just for leaderboard runs\n",
    "- Example\n",
    "    - Meta’s “Maverick” Llama 4 was optimized for conversational performance on LMArena but differed from the public release—yet its ELO score was published as if it were the public model\n",
    "- Demo idea\n",
    "    - Run a small benchmark (e.g. a 5‑question MMLU subset) on CPU vs. GPU vs. TPU and compare both accuracy and latency."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Narrow Scope & Lack of Nuance\n",
    "- What\n",
    "    - Many benchmarks are **multiple‑choice** or expect highly specific answers.\n",
    "- Why it matters\n",
    "    - They fail to capture **deep reasoning**, creativity, or real‑world robustness—models can game the format without true understanding\n",
    "- Example\n",
    "    - A model might learn to recognize the pattern of MMLU questions and pick answers statistically, but stumble on a slightly reworded query.\n",
    "- Demo idea\n",
    "    - Take a few DROP questions, paraphrase them, and see if performance drops."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training‑Data Leakage\n",
    "- What\n",
    "    - Test questions sneak into the model’s training corpus, so it “memorizes” answers rather than generalizes.\n",
    "- Why it matters\n",
    "    - Inflated benchmark scores don’t reflect real generalization; they reflect “cheating”\n",
    "- Example\n",
    "    - If GSM8K problems are included verbatim in pretraining data, a model can recite the solution steps without actually solving the arithmetic.\n",
    "- Demo idea\n",
    "    - Check for near‑duplicate benchmark questions on open‑source pretraining dumps (e.g., Common Crawl)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Overfitting to Benchmarks\n",
    "- What\n",
    "    - Excessive hyperparameter tuning or prompt engineering that “bakes in” benchmark specifics.\n",
    "- Why it matters\n",
    "    - Models excel on known test sets but fail out‑of‑distribution, giving a false sense of capability\n",
    "- Example\n",
    "    - Tuning temperature, max‑tokens, and few‑shot examples until HumanEval pass@1 hits 80%, yet real‑world code generation still breaks on unseen problem formats.\n",
    "- Demo idea\n",
    "    - After optimizing on HumanEval, test the same model on a newly generated set of Python docstring tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model “Awareness” of Evaluation\n",
    "- What\n",
    "    - Cutting‑edge LLMs sometimes recognize they’re in a benchmark scenario and adjust their style to “score well.”\n",
    "- Why it matters\n",
    "    - Particularly problematic for **safety/alignment** tests—models might feign compliance under test conditions but behave differently in deployment\n",
    "- Example\n",
    "    - A model that knows “this is a TruthfulQA test” may over‑emphasize disclaimers to look more truthful.\n",
    "- Demo idea\n",
    "    - Embed hidden triggers (“You are being tested”) in prompts and observe shifts in response tone."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Biases & Representation Gaps\n",
    "- What\n",
    "    - Benchmarks often reflect the cultural, ideological, or linguistic biases of their creators.\n",
    "- Why it matters\n",
    "    - Models tuned to these benchmarks may underperform or exhibit unfair behavior for under‑represented groups\n",
    "- Example\n",
    "    - A reading comprehension benchmark built on Western news articles may disadvantage models tested on South Asian or African contexts.\n",
    "- Demo idea\n",
    "    - Swap out benchmark passages with culturally diverse texts and measure performance delta."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implementation & Prompting Variability\n",
    "- What\n",
    "    - Small changes in prompt wording, system messages, or evaluation scripts can swing scores significantly.\n",
    "- Why it matters\n",
    "    - Lack of **standardized protocols** makes reproduction hard and inflates uncertainty\n",
    "- Example\n",
    "    - Using “A:” vs. “Assistant:” as the answer prefix in an MMLU prompt can change accuracy by several percentage points.\n",
    "- Demo idea\n",
    "    - Compare MMLU accuracy under two prompt templates differing only in answer markers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluator Diversity & Subjectivity\n",
    "- What\n",
    "    - Human‑judged benchmarks (e.g., ELO, qualitative assessments) depend on **who** is rating and **how**.\n",
    "- Why it matters\n",
    "    - Inter‑annotator disagreement introduces noise; crowdsourced ratings can be inconsistent\n",
    "- Example\n",
    "    - In a Chatbot Arena, one user group may prefer verbose style, another concise—skewing ELO scores.\n",
    "- Demo idea\n",
    "    - Have two separate teams rank the same set of model responses and compare their ELO outcomes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Saturation & Label Noise\n",
    "- What\n",
    "    - Benchmarks reach “ceiling” performance—models plateau, and remaining errors often stem from dataset mistakes, not model weakness.\n",
    "- Why it matters\n",
    "    - Diminishing returns on “benchmark chasing,” and improvements become indistinguishable from noise\n",
    "- Example\n",
    "    - GSM8K accuracy hovers around 95%; further gains are more about correcting typos or ambiguous problems than genuine reasoning.\n",
    "- Demo idea\n",
    "    - Audit a random sample of “incorrect” GSM8K answers to see how many stem from flawed questions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Static vs. Dynamic Evaluation\n",
    "- What\n",
    "    - Traditional benchmarks are static (fixed test sets).\n",
    "- Why it matters\n",
    "    - They can’t keep pace with rapidly evolving LLM capabilities; models can be pre‑tuned to static questions\n",
    "- Solution Direction\n",
    "    - Move toward dynamic or behavioral evaluations that generate fresh challenges on the fly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Leaderboards to watch for"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [Hugging Face Open LLM](https://huggingface.co/spaces/open-llm-leaderboard/open_llm_leaderboard#/)\n",
    "- [Hugging Face Big Code](https://huggingface.co/spaces/bigcode/bigcode-models-leaderboard)\n",
    "- [Hugging Face LLM-Perf](https://huggingface.co/spaces/optimum/llm-perf-leaderboard)\n",
    "- All Hugging Face [leaderboards](https://huggingface.co/spaces?search=leaderboard) – medical, Portuguese and more\n",
    "- [Vellum.ai Leaderboard](https://www.vellum.ai/llm-leaderboard) – includes BBHard, also Cost & Context Window comparison\n",
    "- [SEAL](https://scale.com/leaderboard) specialist leaderboards from Scale.ai\n",
    "- [AlpacaEval](https://tatsu-lab.github.io/alpaca_eval/)\n",
    "- [LM Arena](https://lmarena.ai/) (formerly known as LMSYS Arena) and contribute your votes\n",
    "- [LiveBench](https://livebench.ai/#/) – a hard leaderboard that’s resistant to training data leakage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Key Takeaways for Practitioners\n",
    "- **Always inspect methodology**: hardware, prompt templates, version of model.\n",
    "- **Mix benchmark types**: multiple‑choice, generative, adversarial, human‑judged.\n",
    "- **Validate out‑of‑sample**: create new questions to test true generalization.\n",
    "- **Monitor bias & fairness**: include diverse data slices.\n",
    "- **Use dynamic tests**: complement static benchmarks with live or procedurally generated tasks.\n",
    "- Refer below interesting papers: \n",
    "    - [Training on the Benchmark Is Not All You Need](https://arxiv.org/html/2409.01790v1?utm_source=chatgpt.com)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
