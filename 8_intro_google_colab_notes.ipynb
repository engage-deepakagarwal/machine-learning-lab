{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "05a841a6",
   "metadata": {},
   "source": [
    "# Introduction to Google Colab\n",
    "- https://colab.research.google.com.\n",
    "- A cloud-based environment that runs Jupyter notebooks, allowing you to write, run, and share Python code in the browser.\n",
    "- Designed for data science, machine learning, and collaborative coding—Colab makes high-powered computing (including GPU/TPU support) accessible without local setup."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c229645",
   "metadata": {},
   "source": [
    "# Key Features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4adf2068",
   "metadata": {},
   "source": [
    "- Run notebooks on remote Google servers, which provide a decent CPU and optional GPU resources.\n",
    "- Multiple Runtime Options\n",
    "- Secrets Management: You can add API keys and other credentials as “secrets” that remain private even when you share your notebook.\n",
    "- The local disk in a Colab session is ephemeral—data stored there will be wiped when the session ends.\n",
    "- Comes with many commonly used libraries pre-installed (e.g., TensorFlow, PyTorch, Keras, NumPy, Pandas), allowing you to get started immediately."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0750fc56",
   "metadata": {},
   "source": [
    "# Multiple Runtime Options\n",
    "\n",
    " **Aspect**                     | **CPU (Central Processing Unit)**                                                                                                                                                     | **GPU (Graphics Processing Unit)**                                                                                                                                       | **TPU (Tensor Processing Unit)**                                                                                                                                                       | **FPGA (Field Programmable Gate Array) *Not available but added for completeness on runtime options***                                                                                                                                                            \n",
    "--------------------------------|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|--------------------------------------------------------------------------------------------------------------------------------------------------------------------------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    " Primary Role in AI Workflows   | – Executes general-purpose code, orchestration, data preprocessing, and control flow in AI pipelines.<br>– Suitable for inference on smaller models or when flexibility is paramount. | – Highly effective for training and inference of deep neural networks due to thousands of parallel cores.<br>– Ideal for data-parallel tasks and batch computations.     | – Optimized for large-scale deep learning training and high-throughput inference, particularly matrix/tensor operations.<br>– Often integrated with TensorFlow and other ML libraries. | – Offers customizable acceleration tailored to specific AI inference tasks.<br>– Useful for low-latency, power-efficient, and real-time inference in edge devices.                                  \n",
    " Architecture & Parallelism     | – A few highly optimized cores with complex control units.<br>– Excellent at handling sequential and diverse tasks.                                                                   | – Thousands of simpler cores that execute many operations in parallel.<br>– Designed for high-throughput arithmetic operations on large data arrays.                     | – Utilizes systolic array architectures and specialized matrix multiply units, enabling massive parallelism specifically for tensor operations.                                        | – Composed of configurable logic blocks that can be tailored to a particular AI algorithm.<br>– Can be reprogrammed for specific parallelism and pipelining needs.                                  \n",
    " Integration with AI Frameworks | – Fully supported by all AI frameworks (TensorFlow, PyTorch, etc.) but may be a bottleneck for training large models.                                                                 | – Standard choice for AI training; well-supported by frameworks via CUDA (NVIDIA) or ROCm (AMD).<br>– Widely used in both research and production environments.          | – Natively integrated with TensorFlow (and available via high-level APIs), offering significant speed-ups for training.<br>– Less flexible for non-ML workloads.                       | – Can be integrated into AI pipelines for specialized inference acceleration.<br>– Requires additional development effort (e.g., using OpenCL or high-level synthesis tools) compared to GPUs/TPUs. \n",
    " Performance & Throughput       | – Lower throughput for matrix-heavy operations compared to GPUs/TPUs.<br>– Suitable for handling control logic and less parallel computations.                                        | – Excellent for high-volume parallel tasks; reduces training time significantly on large datasets and complex models.                                                    | – Provides high throughput for deep learning workloads, often outperforming GPUs on specific tensor computations.<br>– Offers energy-efficient performance for targeted ML tasks.      | – Can achieve very low latency and high efficiency when optimized for a given model; performance depends on custom design.                                                                          \n",
    " Energy Efficiency & Cost       | – Versatile but less energy-efficient for AI-specific operations.<br>– Generally higher cost per operation for large-scale training.                                                  | – More energy efficient for parallel numerical tasks; cost-effective for accelerating training and inference.<br>– Widely available in consumer and data center markets. | – Designed to deliver maximum throughput per watt for tensor operations; optimized for cost-effectiveness in deep learning workloads.                                                  | – High initial development cost with potentially lower operational power consumption when finely tuned for a specific task.                                                                         \n",
    " Use Cases for AI Engineers     | – Running model orchestration, data preprocessing, and handling varied computational tasks.<br>– Suitable for inference on simpler models or prototyping.                             | – Training complex deep learning models, batch processing, real-time inference in high-performance computing environments.                                               | – Training large-scale neural networks, rapid prototyping with high throughput, and specialized deep learning tasks in production.                                                     | – Deploying custom, low-latency inference accelerators, edge AI applications, and scenarios where bespoke hardware optimization is required.|"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaab634d",
   "metadata": {},
   "source": [
    "# Practical Usage & Tips\n",
    "- Use “magic” commands (prefixed with `%` or `%%`) to manage the runtime environment, list available commands, or inspect system resources.\n",
    "- Install additional packages using `!pip install package-name`.\n",
    "- Note that installations persist only for the duration of your session, so you may need to reinstall in a new session.\n",
    "- To access your Drive files, run:\n",
    "    `from google.colab import drive\n",
    "    drive.mount('/content/drive')`\n",
    "- Monitor your usage with the “View resources” option in the Colab interface.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
