{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a61cb4be",
   "metadata": {},
   "source": [
    "# Hugging Face\n",
    "\n",
    "- https://huggingface.co/\n",
    "- Open source resource for data science, machine and LLM engineering."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8922b2a8",
   "metadata": {},
   "source": [
    "## Platform\n",
    "\n",
    "  - `Models`: Over 800,000 open source models available for diverse tasks.\n",
    "  - `Datasets`: More than 200,000 datasets for various problems, comparable to resources like Kaggle.\n",
    "  - `Spaces`: A platform to host and share apps (commonly built using Gradio or Streamlit) including leaderboards for comparing LLMs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21185612",
   "metadata": {},
   "source": [
    "## Hugging Face Libraries\n",
    "- `hub`\n",
    "  - A library to log in to Hugging Face, download, and upload models and datasets.\n",
    "- `datasets`\n",
    "  - Provides immediate access to Hugging Face’s vast data repositories.\n",
    "- `transformers`\n",
    "  - Wraps LLMs (with underlying PyTorch or TensorFlow implementations) so that model inference or training is performed locally rather than via cloud APIs.\n",
    "- `peft` (Parameter Efficient Fine Tuning)\n",
    "  - Tools to fine-tune LLMs efficiently without handling all billions of parameters (includes methods like LoRA).\n",
    "- `trl` (Transformer Reinforcement Learning)\n",
    "  - Encompasses techniques such as reward modeling, proximal policy optimization (PPO), and supervised fine-tuning (SFT) – key to advancements like ChatGPT.\n",
    "- `accelerate`\n",
    "  - Facilitates distributed computing for training and inference, scaling across multiple GPUs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d6cb1af",
   "metadata": {},
   "source": [
    "# API Models for Hugging Face"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7453c004",
   "metadata": {},
   "source": [
    "![alt text](images/api_levels_hugging_face.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dac447d5",
   "metadata": {},
   "source": [
    "# Hosting Models in Hugging Face"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fff47f91",
   "metadata": {},
   "source": [
    "- The `InferenceClient` is the unified, Pythonic entry-point to run model inference on Hugging Face’s free Inference API, self-hosted Endpoints, or third-party providers (e.g. OpenAI, Replicate, Fal-AI)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d909d225",
   "metadata": {},
   "source": [
    "![alt text](images/model_hosting_hugging_face.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbb6a5a0",
   "metadata": {},
   "source": [
    "![alt text](images/create_inference_endpoint_hugging_face.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e668db1",
   "metadata": {},
   "source": [
    "![alt text](images/hugging_face_inference_endpoint_usage.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4e1ea2b",
   "metadata": {},
   "source": [
    "![alt text](images/hugging_face_inference_client_usage.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
