{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b9f36bbc",
   "metadata": {},
   "source": [
    "# Motivation\n",
    "- **Hallucination & over‑confidence**: Modern LLMs (e.g. GPT‑3.5, GPT‑4) often emit plausible‑looking but incorrect answers, with no in‑built calibration or access to training data or token‑probabilities.\n",
    "\n",
    "- **High‑stakes need**: In critical applications (legal drafting, medical advice, automated evaluation), users must know when to trust an LLM’s response.\n",
    "\n",
    "- **Black‑box constraint**: Cannot modify LLM training or access internal probabilities—must wrap around API calls."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "585d356f",
   "metadata": {},
   "source": [
    "# BSDetector Method Overview\n",
    "- BSDetector produces, alongside any LLM answer, a numeric confidence score C ∈ [0,1] by combining two complementary signals:\n",
    "\n",
    "- **Observed Consistency (O)** – an extrinsic measure of how much the model’s various plausible outputs contradict the original answer.\n",
    "\n",
    "- **Self‑reflection Certainty (S)** – an intrinsic measure obtained by prompting the LLM to introspectively judge its own answer’s correctness.\n",
    "\n",
    "Overall score:\n",
    "    `C = β·O + (1–β)·S`<br>\n",
    "with β a tunable trade‑off parameter."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "652027ca",
   "metadata": {},
   "source": [
    "![alt text](<papers/Quantifying Uncertainty in Answers from any Language Model and Enhancing their Trustworthiness/images/BSDetectorPipeline.png>)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72940791",
   "metadata": {},
   "source": [
    "# Key Drawbacks of Raw Embedding Similarity Metrics\n",
    "\n",
    "A Simple Intuitive Example<br>\n",
    "- **Sentence A**: \"The quick brown fox jumps over the lazy dog.\"<br>\n",
    "- **Sentence B**: \"A swift auburn fox leaped above the lethargic canine.\"\n",
    "\n",
    "- True semantic relationship:\n",
    "Both sentences describe exactly the same scene—only the word choices differ.\n",
    "\n",
    "- Embedding‑space outcome:\n",
    "  - A raw cosine‑similarity model might score them around 0.65–0.75 (out of 1), which looks only moderately similar—even though to a human reader they’re identical in meaning.\n",
    "  - This drop occurs because the model’s vectors are sensitive to different token distributions (\"quick\" vs. \"swift\", \"brown\" vs. \"auburn\", etc.) and to how often those specific words appeared during pretraining.\n",
    "  - High‑frequency words tend to cluster differently than low‑frequency ones, causing point estimates of similarity to systematically over‑ or under‑estimate human judgments of meaning closeness. This means that even two semantically identical phrases can appear distant simply because one uses more common words than the other\n",
    "  - Small stylistic tweaks—like adding an adverb or swapping a single adjective—can shift an embedding enough to drop cosine similarity below a threshold, despite no real change in meaning. \n",
    "- Why this matters for uncertainty estimation:\n",
    "If you treated that moderate cosine score as evidence of “uncertainty” in meaning, you’d incorrectly conclude the model is unsure about the answer, when in fact it’s just reworded it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53ed258d",
   "metadata": {},
   "source": [
    "# A better strategy? NLI\n",
    "## Summary\n",
    "- Natural Language Inference (NLI), also called Recognizing Textual Entailment (RTE), is the task of determining whether a **hypothesis** sentence is **entailed by**, **contradicts**, or is **neutral** with respect to a **premise** sentence.\n",
    "- An NLI model outputs a probability distribution over these three labels, enabling us to judge whether two pieces of text agree in meaning, oppose each other, or have no definite relationship.\n",
    "- The NLI framework is particularly powerful for semantic comparison because it focuses on logical relationships rather than surface‑level word overlap or embedding proximity.\n",
    "\n",
    "## What Is NLI?\n",
    "NLI frames meaning comparison as a three‑way classification task:\n",
    "- **Entailment**: The premise logically implies the hypothesis.\n",
    "- **Contradiction**: The premise and hypothesis cannot both be true simultaneously.\n",
    "- **Neutral**: Neither entailment nor contradiction—there’s no clear inference in either direction.\n",
    "\n",
    "## Why Use NLI?\n",
    "- **Logic‑focused**: Goes beyond surface‐form similarity to capture true semantic agreement or opposition\n",
    "- **Graded confidence**: By producing probabilities for each label, NLI gives a fine‑grained measure of how strongly two sentences align or conflict.\n",
    "- **Robust to rephrasing**: Ignores trivial wording differences that might fool embedding‑based metrics.\n",
    "\n",
    "## How Does an NLI Model Work?\n",
    "![alt text](<papers/Quantifying Uncertainty in Answers from any Language Model and Enhancing their Trustworthiness/images/How NLI works.png>)\n",
    "\n",
    "## Simple Examples\n",
    "![alt text](<papers/Quantifying Uncertainty in Answers from any Language Model and Enhancing their Trustworthiness/images/Examples of NLI.png>)\n",
    "\n",
    "## Intuitive Key-Takeaways\n",
    "By using an NLI classifier, we move from brittle string‑matching or embedding‑distance heuristics to a more robust, logic‑driven measure of semantic consistency—key for assessing uncertainty in model outputs or flagging truly conflicting responses."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4da85cf",
   "metadata": {},
   "source": [
    "# Effect of different sentence similarity metrics\n",
    "![alt text](<papers/Quantifying Uncertainty in Answers from any Language Model and Enhancing their Trustworthiness/images/Effect of different sentence similarity metrics.png>)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4552dc3b",
   "metadata": {},
   "source": [
    "# Observed Consistency\n",
    "- The authors propose to quantify the uncertainty of a language model’s answer by sampling multiple outputs and then measuring how semantically “aligned” each sampled answer is with the original (reference) answer.\n",
    "- Prompt Template\n",
    "![alt text](<papers/Quantifying Uncertainty in Answers from any Language Model and Enhancing their Trustworthiness/images/Prompt Template for Observed Consistency.png>)\n",
    " \n",
    "- **Diverse sampling**: Re‑invoke the LLM k times (e.g. k = 5) with a Chain‑of‑Thought prompt and high temperature to yield answers {y₁…yₖ}.\n",
    "\n",
    "- **Semantic contradiction scoring**: For each sample yᵢ, measure contradiction vs. original y via a Natural Language Inference (NLI) model:\n",
    "  - Compute pᵢ = probability(“contradiction”) for the pair (yᵢ,y) (averaging both input orders).\n",
    "  - Also include an indicator rᵢ = 1[yᵢ = y] to stabilize on closed‑form tasks.\n",
    "  - Similarity score oᵢ = α·(1–pᵢ) + (1–α)·rᵢ.\n",
    "- Aggregate: O = meanᵢ oᵢ.\n",
    "  - High O means sampled answers largely agree with y; low O signals many contradictions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bf656f8",
   "metadata": {},
   "source": [
    "### Prompt template for observed consistency\n",
    "![alt text](<papers/Quantifying Uncertainty in Answers from any Language Model and Enhancing their Trustworthiness/images/Prompt Template for Observed Consistency.png>)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e0bce4dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To run this demo, first install dependencies:\n",
    "# pip install openai transformers torch\n",
    "\n",
    "import os\n",
    "import openai\n",
    "from dotenv import load_dotenv\n",
    "from transformers import pipeline\n",
    "from huggingface_hub import login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3cb7d18c",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv(override=True)\n",
    "\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "login(token=os.getenv(\"HUGGINGFACE_TOKEN\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "084800c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-large-mnli were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: Who was the only survivor of Titanic?\n",
      "Original Answer: The only survivor of the Titanic disaster was not a single individual, as there were actually many survivors. However, if you are referring to a specific person often highlighted in stories about the Titanic, it could be referring to individuals like Molly Brown, who became famous for her efforts to help others during the disaster. \n",
      "\n",
      "If you meant the last living survivor, that was Millvina Dean, who was just a few months old at the time of the sinking in 1912. She passed away in 2009. If you have a specific context in mind, please clarify!\n",
      "\n",
      "1. The Titanic sank on April 15, 1912, during its maiden voyage...   o_1 = 0.900\n",
      "2. To determine the only survivor of the Titanic, we need to cl...   o_2 = 0.900\n",
      "3. To clarify the question about the only survivor of the Titan...   o_3 = 0.900\n",
      "4. The only survivor of the Titanic is a common misconception. ...   o_4 = 0.627\n",
      "5. The Titanic, which sank on April 15, 1912, had many passenge...   o_5 = 0.900\n",
      "\n",
      "Observed Consistency score O = 0.845\n"
     ]
    }
   ],
   "source": [
    "def get_original_answer(question: str) -> str:\n",
    "    resp = openai.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=[{\"role\": \"user\", \"content\": question}],\n",
    "        temperature=0.0,\n",
    "    )\n",
    "    return resp.choices[0].message.content.strip()\n",
    "\n",
    "# Function to sample k diverse responses with CoT prompt at temperature=1.0\n",
    "def sample_responses(question: str, k: int = 5, cot_prompt: str = \"Think step-by-step:\") -> list[str]:\n",
    "    samples = []\n",
    "    for _ in range(k):\n",
    "        prompt = f\"{question}\\n{cot_prompt}\"\n",
    "        resp = openai.chat.completions.create(\n",
    "            model=\"gpt-4o-mini\",\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            temperature=1.0,\n",
    "        )\n",
    "        samples.append(resp.choices[0].message.content.strip())\n",
    "    return samples\n",
    "\n",
    "# Load NLI model for contradiction detection\n",
    "nli = pipeline(\"text-classification\", model=\"roberta-large-mnli\")\n",
    "\n",
    "def contradiction_prob(hypothesis: str, premise: str) -> float:\n",
    "    \"\"\"\n",
    "    Returns the probability that 'hypothesis' contradicts 'premise',\n",
    "    averaging both input orders to mitigate positional bias.\n",
    "    \"\"\"\n",
    "    def p_contra(text_pair: str) -> float:\n",
    "        # Use the NLI model to get the contradiction score\n",
    "        # The model returns a list of scores for 'entailment', 'neutral', and 'contradiction'\n",
    "        result = nli(text_pair)\n",
    "\n",
    "        # For roberta-large-mnli, we need to find the contradiction probability\n",
    "        # The model returns probabilities for all classes (contradiction, entailment, neutral)\n",
    "\n",
    "        if isinstance(result, list) and len(result) > 0:\n",
    "            if isinstance(result[0], dict) and 'label' in result[0]:\n",
    "                # If the model returns the highest probability class only\n",
    "                if result[0]['label'].lower() == \"contradiction\":\n",
    "                    return result[0]['score']\n",
    "                return 0.0\n",
    "            else:\n",
    "                # If model returns probabilities for all classes\n",
    "                for item in result:\n",
    "                    if item['label'].lower() == \"contradiction\":\n",
    "                        return item['score']\n",
    "        \n",
    "        # If using a different format (e.g., with pipeline that returns all class probabilities)\n",
    "        try:\n",
    "            probs = {item['label'].lower(): item['score'] for item in result}\n",
    "            return probs.get('contradiction', 0.0)\n",
    "        except:\n",
    "            # Fallback\n",
    "            return 0.0\n",
    "    \n",
    "    p1 = p_contra(f\"{hypothesis} </s> {premise}\")\n",
    "    p2 = p_contra(f\"{premise} </s> {hypothesis}\")\n",
    "    return (p1 + p2) / 2.0\n",
    "\n",
    "# 5. Compute Observed Consistency\n",
    "def observed_consistency(question: str, k: int = 5, alpha: float = 0.9):\n",
    "    # original answer\n",
    "    y = get_original_answer(question)\n",
    "    # sampled alternatives\n",
    "    samples = sample_responses(question, k=k)\n",
    "    # compute o_i for each sample\n",
    "    o_scores = []\n",
    "    for yi in samples:\n",
    "        p_c = contradiction_prob(yi, y)\n",
    "        sim = 1.0 - p_c\n",
    "        ri = 1.0 if yi.strip() == y.strip() else 0.0\n",
    "        o_i = alpha * sim + (1.0 - alpha) * ri\n",
    "        o_scores.append(o_i)\n",
    "    O = sum(o_scores) / len(o_scores)\n",
    "    return y, samples, o_scores, O\n",
    "\n",
    "# 6. Demo run\n",
    "question = \"Who was the only survivor of Titanic?\"\n",
    "original, variants, scores, O_score = observed_consistency(question)\n",
    "\n",
    "print(f\"Question: {question}\")\n",
    "print(f\"Original Answer: {original}\\n\")\n",
    "for idx, (v, sc) in enumerate(zip(variants, scores), 1):\n",
    "    print(f\"{idx}. {v[:60]}...   o_{idx} = {sc:.3f}\")\n",
    "print(f\"\\nObserved Consistency score O = {O_score:.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d8e891f",
   "metadata": {},
   "source": [
    "# Self‑reflection Certainty\n",
    "- Like CoT prompting, self-reflection allows the LLM to employ additional computation to reason more deeply about the correctness of its answer.\n",
    "- To specifically calculate self-reflection certainty, we prompt the LLM to state how confident it is that its original answer was correct.\n",
    "- LLMs to rate their confidence numerically on a continuous scale (0-100) tended to always yield overly high scores (> 90). Instead we ask the LLM to rate its confidence in its original answer via multiple follow-up questions each on a multiple-choice (e.g. 3-way) scale.\n",
    "- Follow‑up multiple‑choice: Prompt the LLM with its original question & answer, asking e.g.:\n",
    "  - \"Is your answer correct? (A) Correct (B) Incorrect (C) I am not sure.\"\n",
    "  - Repeat with varied wording.\n",
    "- Scoring: Map A→1.0, B→0.0, C→0.5, average across rounds → S.\n",
    "  - Captures the model’s own calibrated judgment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feb7f44a",
   "metadata": {},
   "source": [
    "### Prompt template for self-reflection consistency\n",
    "![alt text](<papers/Quantifying Uncertainty in Answers from any Language Model and Enhancing their Trustworthiness/images/Prompt Template for Self-Reflection Consistency.png>)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "acfd6d5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: Who was the only survivor of Titanic?\n",
      "Original Answer: The only survivor of the Titanic disaster was not a single individual, but rather there were several survivors. However, if you are referring to a notable survivor, one of the most famous is Molly Brown, often referred to as \"The Unsinkable Molly Brown.\" She was known for her efforts to help others during the disaster and for her later activism. \n",
      "\n",
      "If you meant to ask about a specific individual who was the last living survivor, that would be Millvina Dean, who was just a few months old at the time of the sinking. She passed away in 2009. \n",
      "\n",
      "If you have a different context in mind, please clarify!\n",
      "\n",
      "Round 1: choice = A, score = 1.0\n",
      "Round 2: choice = A, score = 1.0\n",
      "Round 3: choice = A, score = 1.0\n",
      "Round 4: choice = A, score = 1.0\n",
      "Round 5: choice = A, score = 1.0\n",
      "\n",
      "Self-reflection Certainty S = 1.000\n"
     ]
    }
   ],
   "source": [
    "def get_original_answer(question: str) -> str:\n",
    "    \"\"\"Fetch the model's original answer at temperature=0.\"\"\"\n",
    "    resp = openai.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=[{\"role\": \"user\", \"content\": question}],\n",
    "        temperature=0.0\n",
    "    )\n",
    "    return resp.choices[0].message.content.strip()\n",
    "\n",
    "def self_reflection_certainty(question: str, answer: str, rounds: int = 5):\n",
    "    \"\"\"\n",
    "    Compute Self-reflection Certainty S by prompting the LLM to judge its own answer.\n",
    "    Returns:\n",
    "      - judgments: list of choices (A/B/C)\n",
    "      - scores: list of numeric scores (A→1.0, B→0.0, C→0.5)\n",
    "      - S: average score\n",
    "    \"\"\"\n",
    "    score_map = {\"A\": 1.0, \"B\": 0.0, \"C\": 0.5}\n",
    "    prompts = [\n",
    "        f\"Question: {question}\\nAnswer: {answer}\\nIs this answer correct? Choose (A) Correct (B) Incorrect (C) I am not sure.\",\n",
    "        f\"Question: {question}\\nAnswer: {answer}\\nHow confident are you in your answer? (A) Correct (B) Incorrect (C) I am not sure.\"\n",
    "    ]\n",
    "    judgments, scores = [], []\n",
    "\n",
    "    for i in range(rounds):\n",
    "        prompt = prompts[i % len(prompts)]\n",
    "        resp = openai.chat.completions.create(\n",
    "            model=\"gpt-4o-mini\",\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            temperature=0.0\n",
    "        )\n",
    "        text = resp.choices[0].message.content.strip()\n",
    "        # Extract letter choice\n",
    "        choice = text[0].upper() if text and text[0].upper() in score_map else None\n",
    "        if choice not in score_map:\n",
    "            # Fallback: scan for any valid letter\n",
    "            for letter in score_map:\n",
    "                if letter in text:\n",
    "                    choice = letter\n",
    "                    break\n",
    "        judgments.append(choice)\n",
    "        scores.append(score_map.get(choice, 0.0))\n",
    "\n",
    "    S = sum(scores) / len(scores) if scores else 0.0\n",
    "    return judgments, scores, S\n",
    "\n",
    "# Demo usage\n",
    "question = \"Who was the only survivor of Titanic?\"\n",
    "original_answer = get_original_answer(question)\n",
    "judgments, numeric_scores, S_score = self_reflection_certainty(question, original_answer)\n",
    "\n",
    "print(f\"Question: {question}\")\n",
    "print(f\"Original Answer: {original_answer}\\n\")\n",
    "for idx, (j, s) in enumerate(zip(judgments, numeric_scores), 1):\n",
    "    print(f\"Round {idx}: choice = {j}, score = {s}\")\n",
    "print(f\"\\nSelf-reflection Certainty S = {S_score:.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edac930f",
   "metadata": {},
   "source": [
    "# Total Trustworthiness score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4655e7d",
   "metadata": {},
   "source": [
    "#### Overall Confidence Estimate\n",
    "![alt text](<papers/Quantifying Uncertainty in Answers from any Language Model and Enhancing their Trustworthiness/images/Overall Confidence Estimate.png>)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "75c13a12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: Who was the only survivor of Titanic?\n",
      "Original Answer: The only survivor of the Titanic disaster was not a single individual, as there were many survivors. However, if you are referring to a notable survivor, one of the most famous is Molly Brown, often referred to as \"The Unsinkable Molly Brown.\" She was known for her efforts to help others during the disaster and her subsequent activism. If you meant a specific individual or a different context, please clarify!\n",
      "Observed Consistency (O): 0.900\n",
      "Self-reflection Certainty (S): 1.000\n",
      "Final BSDetector Confidence (C): 0.933\n"
     ]
    }
   ],
   "source": [
    "# === Final Confidence ===\n",
    "def bsdetector_confidence(question: str,\n",
    "                          k: int = 5,\n",
    "                          alpha: float = 0.9,\n",
    "                          beta: float = 0.5) -> float:\n",
    "    \"\"\"\n",
    "    Compute final BSDETECTOR confidence score C = beta*O + (1-beta)*S.\n",
    "    \"\"\"\n",
    "    original = get_original_answer(question)\n",
    "    O = observed_consistency(question, k=k, alpha=alpha)\n",
    "    score_O = O[3]  # Extract the observed consistency score\n",
    "    S = self_reflection_certainty(question, original, rounds=2)\n",
    "    score_S = S[2]  # Extract the self-reflection certainty score\n",
    "    C = beta * score_O + (1 - beta) * score_S\n",
    "    return C\n",
    "\n",
    "q = \"Who was the only survivor of Titanic?\"\n",
    "y = get_original_answer(q)\n",
    "O = observed_consistency(q)\n",
    "score_O = O[3]  # Extract the observed consistency score\n",
    "S = self_reflection_certainty(q, y)\n",
    "score_S = S[2]  # Extract the self-reflection certainty score\n",
    "C = bsdetector_confidence(q)\n",
    "print(f\"Question: {q}\")\n",
    "print(f\"Original Answer: {y}\")\n",
    "print(f\"Observed Consistency (O): {score_O:.3f}\")\n",
    "print(f\"Self-reflection Certainty (S): {score_S:.3f}\")\n",
    "print(f\"Final BSDetector Confidence (C): {C:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95ecf068",
   "metadata": {},
   "source": [
    "# Experimental Validation\n",
    "- **Benchmarks**: Math word problems (GSM8K, SVAMP), commonsense QA (CSQA), open trivia (TriviaQA).\n",
    "- **Baselines**:\n",
    "  - Likelihood‑based uncertainty (token prob).\n",
    "  - Temperature‑sampling variance.\n",
    "  - Prior self‑reflection methods.\n",
    "- **Calibration (AUROC)**: BSDETECTOR achieves AUROC ≈ 0.77–0.95, outperforming all baselines by large margins (e.g. on GSM8K: 0.867→0.951; SVAMP: 0.936).\n",
    "- **Improved answer selection**: Generating 5 candidates & choosing the one with highest C raises accuracy (e.g. GSM8K from 47 %→70 %, SVAMP 75 %→82 %).\n",
    "- **Reliable LLM‑based evaluation**:\n",
    "  - Human‑in‑loop: route low‑C evaluations for human review, boosting overall evaluation accuracy vs. random\n",
    "  - Fully automated: dropping bottom‑20 % by C yields evaluation scores with lower variance and bias."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2640ec7f",
   "metadata": {},
   "source": [
    "# Applications\n",
    "- **Adaptive routing**: \"I don’t know\" or human‑fallback when C low.\n",
    "- **Answer boosting**: Select among multiple LLM replies via highest C.\n",
    "- **Trustworthy automated evaluation**: Filter or flag low‑confidence judgments in LLM evaluators.\n",
    "- **Real‑world demo**: Legal drafting assistant that flags low‑C clauses for attorney review."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0d56a7a",
   "metadata": {},
   "source": [
    "# Limitations & Future Work\n",
    "- Compute overhead: Requires multiple extra LLM calls and NLI passes.\n",
    "- Domain generalization: NLI and prompts tuned on QA—other tasks may need adaptation.\n",
    "- Black‑box constraints: Cannot leverage token‑level signals for GPT‑3.5; reliant on prompt engineering\n",
    "- Future directions:\n",
    "  - Reduce sampling costs (e.g. via smarter subset selection).\n",
    "  - Extend to non‑QA tasks (summarization, dialogue).\n",
    "  - Explore alternative contradiction detectors beyond NLI."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1ab3363",
   "metadata": {},
   "source": [
    "# Key takeaway\n",
    "BSDetector offers a simple, general wrapper to endow any black‑box LLM with well‑calibrated confidence estimates—enabling safer, more trustworthy AI deployments."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
